{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9057a343-f85e-492c-b610-fa4b156142fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training set Accuracy:92.778%\n",
      "Validation set Accuracy:91.47999999999999%\n",
      "Testing set Accuracy:91.97%\n",
      "\n",
      "\n",
      "--------------SVM-------------------\n",
      "\n",
      "\n",
      "--For Linear--\n",
      "SVM Training done\n",
      "\n",
      "Train set Accuracy: 99.72999999999999%\n",
      "Validation set Accuracy: 91.64%\n",
      "Test set Accuracy: 92.16%\n",
      "\n",
      "\n",
      "--For RBF with gamma 1--\n",
      "SVM Training done\n",
      "\n",
      "Train set Accuracy: 100.0%\n",
      "Validation set Accuracy: 10.040000000000001%\n",
      "Test set Accuracy: 11.49%\n",
      "\n",
      "\n",
      "--For RBF--\n",
      "SVM Training done\n",
      "\n",
      "Train set Accuracy: 98.69%\n",
      "Validation set Accuracy: 96.0%\n",
      "Test set Accuracy: 96.27%\n",
      "\n",
      "\n",
      "--For RBF with varying C--\n",
      "C=1\n",
      "SVM Training done\n",
      "\n",
      "Train set Accuracy: 98.69%\n",
      "Validation set Accuracy: 96.0%\n",
      "Test set Accuracy: 96.27%\n",
      "\n",
      "\n",
      "C=10\n",
      "SVM Training done\n",
      "\n",
      "Train set Accuracy: 100.0%\n",
      "Validation set Accuracy: 96.88%\n",
      "Test set Accuracy: 96.93%\n",
      "\n",
      "\n",
      "C=20\n",
      "SVM Training done\n",
      "\n",
      "Train set Accuracy: 100.0%\n",
      "Validation set Accuracy: 96.88%\n",
      "Test set Accuracy: 96.92%\n",
      "\n",
      "\n",
      "C=30\n",
      "SVM Training done\n",
      "\n",
      "Train set Accuracy: 100.0%\n",
      "Validation set Accuracy: 96.88%\n",
      "Test set Accuracy: 96.92%\n",
      "\n",
      "\n",
      "C=40\n",
      "SVM Training done\n",
      "\n",
      "Train set Accuracy: 100.0%\n",
      "Validation set Accuracy: 96.88%\n",
      "Test set Accuracy: 96.92%\n",
      "\n",
      "\n",
      "C=50\n",
      "SVM Training done\n",
      "\n",
      "Train set Accuracy: 100.0%\n",
      "Validation set Accuracy: 96.88%\n",
      "Test set Accuracy: 96.92%\n",
      "\n",
      "\n",
      "C=60\n",
      "SVM Training done\n",
      "\n",
      "Train set Accuracy: 100.0%\n",
      "Validation set Accuracy: 96.88%\n",
      "Test set Accuracy: 96.92%\n",
      "\n",
      "\n",
      "C=70\n",
      "SVM Training done\n",
      "\n",
      "Train set Accuracy: 100.0%\n",
      "Validation set Accuracy: 96.88%\n",
      "Test set Accuracy: 96.92%\n",
      "\n",
      "\n",
      "C=80\n",
      "SVM Training done\n",
      "\n",
      "Train set Accuracy: 100.0%\n",
      "Validation set Accuracy: 96.88%\n",
      "Test set Accuracy: 96.92%\n",
      "\n",
      "\n",
      "C=90\n",
      "SVM Training done\n",
      "\n",
      "Train set Accuracy: 100.0%\n",
      "Validation set Accuracy: 96.88%\n",
      "Test set Accuracy: 96.92%\n",
      "\n",
      "\n",
      "C=100\n",
      "SVM Training done\n",
      "\n",
      "Train set Accuracy: 100.0%\n",
      "Validation set Accuracy: 96.88%\n",
      "Test set Accuracy: 96.92%\n",
      "\n",
      "\n",
      "\n",
      " Training set Accuracy:93.108%\n",
      "Validation set Accuracy:92.36999999999999%\n",
      "Testing set Accuracy:92.53%\n"
     ]
    }
   ],
   "source": [
    "# %load script.py\n",
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "from scipy.optimize import minimize\n",
    "from sklearn import svm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def preprocess():\n",
    "    \"\"\" \n",
    "     Input:\n",
    "     Although this function doesn't have any input, you are required to load\n",
    "     the MNIST data set from file 'mnist_all.mat'.\n",
    "\n",
    "     Output:\n",
    "     train_data: matrix of training set. Each row of train_data contains \n",
    "       feature vector of a image\n",
    "     train_label: vector of label corresponding to each image in the training\n",
    "       set\n",
    "     validation_data: matrix of training set. Each row of validation_data \n",
    "       contains feature vector of a image\n",
    "     validation_label: vector of label corresponding to each image in the \n",
    "       training set\n",
    "     test_data: matrix of training set. Each row of test_data contains \n",
    "       feature vector of a image\n",
    "     test_label: vector of label corresponding to each image in the testing\n",
    "       set\n",
    "    \"\"\"\n",
    "\n",
    "    mat = loadmat('mnist_all.mat')  # loads the MAT object as a Dictionary\n",
    "\n",
    "    n_feature = mat.get(\"train1\").shape[1]\n",
    "    n_sample = 0\n",
    "    for i in range(10):\n",
    "        n_sample = n_sample + mat.get(\"train\" + str(i)).shape[0]\n",
    "    n_validation = 1000\n",
    "    n_train = n_sample - 10 * n_validation\n",
    "\n",
    "    # Construct validation data\n",
    "    validation_data = np.zeros((10 * n_validation, n_feature))\n",
    "    for i in range(10):\n",
    "        validation_data[i * n_validation:(i + 1) * n_validation, :] = mat.get(\"train\" + str(i))[0:n_validation, :]\n",
    "\n",
    "    # Construct validation label\n",
    "    validation_label = np.ones((10 * n_validation, 1))\n",
    "    for i in range(10):\n",
    "        validation_label[i * n_validation:(i + 1) * n_validation, :] = i * np.ones((n_validation, 1))\n",
    "\n",
    "    # Construct training data and label\n",
    "    train_data = np.zeros((n_train, n_feature))\n",
    "    train_label = np.zeros((n_train, 1))\n",
    "    temp = 0\n",
    "    for i in range(10):\n",
    "        size_i = mat.get(\"train\" + str(i)).shape[0]\n",
    "        train_data[temp:temp + size_i - n_validation, :] = mat.get(\"train\" + str(i))[n_validation:size_i, :]\n",
    "        train_label[temp:temp + size_i - n_validation, :] = i * np.ones((size_i - n_validation, 1))\n",
    "        temp = temp + size_i - n_validation\n",
    "\n",
    "    # Construct test data and label\n",
    "    n_test = 0\n",
    "    for i in range(10):\n",
    "        n_test = n_test + mat.get(\"test\" + str(i)).shape[0]\n",
    "    test_data = np.zeros((n_test, n_feature))\n",
    "    test_label = np.zeros((n_test, 1))\n",
    "    temp = 0\n",
    "    for i in range(10):\n",
    "        size_i = mat.get(\"test\" + str(i)).shape[0]\n",
    "        test_data[temp:temp + size_i, :] = mat.get(\"test\" + str(i))\n",
    "        test_label[temp:temp + size_i, :] = i * np.ones((size_i, 1))\n",
    "        temp = temp + size_i\n",
    "\n",
    "    # Delete features which don't provide any useful information for classifiers\n",
    "    sigma = np.std(train_data, axis=0)\n",
    "    index = np.array([])\n",
    "    for i in range(n_feature):\n",
    "        if (sigma[i] > 0.001):\n",
    "            index = np.append(index, [i])\n",
    "    train_data = train_data[:, index.astype(int)]\n",
    "    validation_data = validation_data[:, index.astype(int)]\n",
    "    test_data = test_data[:, index.astype(int)]\n",
    "\n",
    "    # Scale data to 0 and 1\n",
    "    train_data /= 255.0\n",
    "    validation_data /= 255.0\n",
    "    test_data /= 255.0\n",
    "\n",
    "    return train_data, train_label, validation_data, validation_label, test_data, test_label\n",
    "\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1.0 / (1.0 + np.exp(-z))\n",
    "\n",
    "\n",
    "def blrObjFunction(initialWeights, *args):\n",
    "    \"\"\"\n",
    "    blrObjFunction computes 2-class Logistic Regression error function and\n",
    "    its gradient.\n",
    "\n",
    "    Input:\n",
    "        initialWeights: the weight vector (w_k) of size (D + 1) x 1 \n",
    "        train_data: the data matrix of size N x D\n",
    "        labeli: the label vector (y_k) of size N x 1 where each entry can be either 0 or 1 representing the label of corresponding feature vector\n",
    "\n",
    "    Output: \n",
    "        error: the scalar value of error function of 2-class logistic regression\n",
    "        error_grad: the vector of size (D+1) x 1 representing the gradient of\n",
    "                    error function\n",
    "    \"\"\"\n",
    "    train_data, labeli = args\n",
    "\n",
    "    n_data = train_data.shape[0]\n",
    "    n_features = train_data.shape[1]\n",
    "    error = 0\n",
    "    error_grad = np.zeros((n_features + 1, 1))\n",
    "\n",
    "    ##################\n",
    "    # YOUR CODE HERE #\n",
    "    ##################\n",
    "    # HINT: Do not forget to add the bias term to your input data\n",
    "   \n",
    "    train_data_bias = np.column_stack((np.ones((n_data, 1)), train_data))\n",
    "    theta_n = sigmoid(np.matmul(train_data_bias, np.matrix(initialWeights).T))\n",
    "  \n",
    "    error = (-1/n_data) * np.sum(np.multiply(labeli, np.log(theta_n)) + np.multiply((1 - labeli), np.log(1 - theta_n)))\n",
    "    error_grad = (1/n_data) * (np.sum(np.multiply((theta_n - labeli), train_data_bias), axis = 0)).T\n",
    "    error_grad = np.asarray(error_grad).flatten()\n",
    "    \n",
    "    #Debug parameters\n",
    "    # print(f'train_data shape: {train_data.shape}')\n",
    "    # print(f'w shape: {initialWeights.shape}')\n",
    "    # print(f'train_data_bias shape: {train_data.shape}')\n",
    "    # print(f'theta_n shape: {theta_n.shape}')\n",
    "    # print(f'error: {error}')\n",
    "    # print(f'error_grad shape: {error_grad.shape}')\n",
    "    \n",
    "    return error, error_grad\n",
    "\n",
    "\n",
    "def blrPredict(W, data):\n",
    "    \"\"\"\n",
    "     blrObjFunction predicts the label of data given the data and parameter W \n",
    "     of Logistic Regression\n",
    "     \n",
    "     Input:\n",
    "         W: the matrix of weight of size (D + 1) x 10. Each column is the weight \n",
    "         vector of a Logistic Regression classifier.\n",
    "         X: the data matrix of size N x D\n",
    "         \n",
    "     Output: \n",
    "         label: vector of size N x 1 representing the predicted label of \n",
    "         corresponding feature vector given in data matrix\n",
    "\n",
    "    \"\"\"\n",
    "    label = np.zeros((data.shape[0], 1))\n",
    "\n",
    "    ##################\n",
    "    # YOUR CODE HERE #\n",
    "    ##################\n",
    "    # HINT: Do not forget to add the bias term to your input data\n",
    "    data_bias = np.column_stack(((np.ones((data.shape[0], 1))), data))\n",
    "    label = np.argmax(sigmoid(np.matmul(data_bias, W)), axis = 1)\n",
    "    label = np.matrix(label).T\n",
    "    \n",
    "    return label\n",
    "\n",
    "def mlrObjFunction(params, *args):\n",
    "    \"\"\"\n",
    "    mlrObjFunction computes multi-class Logistic Regression error function and\n",
    "    its gradient.\n",
    "\n",
    "    Input:\n",
    "        initialWeights_b: the weight vector of size (D + 1) x 10\n",
    "        train_data: the data matrix of size N x D\n",
    "        labeli: the label vector of size N x 1 where each entry can be either 0 or 1\n",
    "                representing the label of corresponding feature vector\n",
    "\n",
    "    Output:\n",
    "        error: the scalar value of error function of multi-class logistic regression\n",
    "        error_grad: the vector of size (D+1) x 10 representing the gradient of\n",
    "                    error function\n",
    "    \"\"\"\n",
    "    train_data, labeli = args\n",
    "    n_data = train_data.shape[0]\n",
    "    n_feature = train_data.shape[1]\n",
    "    error = 0\n",
    "    error_grad = np.zeros((n_feature + 1, n_class))\n",
    "\n",
    "    ##################\n",
    "    # YOUR CODE HERE #\n",
    "    ##################\n",
    "    # HINT: Do not forget to add the bias term to your input data\n",
    "    initialWeights = params.reshape((n_feature+1, n_class))\n",
    "    train_data_bias = np.column_stack((np.ones((n_data, 1)), train_data))\n",
    "\n",
    "    #Softmax function\n",
    "    num = np.exp(np.matmul(train_data_bias, initialWeights))\n",
    "    dem = np.sum(num, axis = 1)\n",
    "    theta_nk = np.divide(num, dem.reshape((dem.shape[0], 1)))\n",
    "    \n",
    "    error = -1*np.sum(np.multiply(labeli, np.log(theta_nk)))\n",
    "    error_grad = np.matmul((theta_nk - labeli).T, train_data_bias).T\n",
    "    error_grad = error_grad.flatten()\n",
    "    \n",
    "    #Debug parameters\n",
    "    # print(f'num shape: {num.shape}')\n",
    "    # print(f'params type: {type(params)}')\n",
    "    # print(f'params shape: {params.shape}')\n",
    "    # print(f'train_data shape: {train_data.shape}')\n",
    "    # print(f'w shape: {initialWeights.shape}')\n",
    "    # print(f'train_data_bias shape: {train_data.shape}')\n",
    "    # print(f'dem shape: {dem.shape}')\n",
    "    # print(f'theta_nk shape: {theta_nk.shape}')\n",
    "    # print(f'error: {error}')\n",
    "    # print(f'error grad type: {type(error_grad)}')\n",
    "    # print(f'error shape: {error_grad.shape}')\n",
    "\n",
    "    return error, error_grad\n",
    "\n",
    "\n",
    "def mlrPredict(W, data):\n",
    "    \"\"\"\n",
    "     mlrObjFunction predicts the label of data given the data and parameter W\n",
    "     of Logistic Regression\n",
    "\n",
    "     Input:\n",
    "         W: the matrix of weight of size (D + 1) x 10. Each column is the weight\n",
    "         vector of a Logistic Regression classifier.\n",
    "         X: the data matrix of size N x D\n",
    "\n",
    "     Output:\n",
    "         label: vector of size N x 1 representing the predicted label of\n",
    "         corresponding feature vector given in data matrix\n",
    "\n",
    "    \"\"\"\n",
    "    label = np.zeros((data.shape[0], 1))\n",
    "\n",
    "    ##################\n",
    "    # YOUR CODE HERE #\n",
    "    ##################\n",
    "    # HINT: Do not forget to add the bias term to your input data\n",
    "    data_bias = np.column_stack((np.ones((data.shape[0], 1)), data))\n",
    "    \n",
    "    #Softmax function\n",
    "    num = np.exp(np.matmul(data_bias, W))\n",
    "    dem = np.sum(num, axis = 1)\n",
    "    theta_nk = np.divide(num, dem.reshape((dem.shape[0], 1)))\n",
    "    \n",
    "    label = np.argmax(theta_nk, axis = 1)\n",
    "    label = np.matrix(label).T\n",
    "    \n",
    "    return label\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Script for Logistic Regression\n",
    "\"\"\"\n",
    "train_data, train_label, validation_data, validation_label, test_data, test_label = preprocess()\n",
    "\n",
    "# number of classes\n",
    "n_class = 10\n",
    "\n",
    "# number of training samples\n",
    "n_train = train_data.shape[0]\n",
    "\n",
    "# number of features\n",
    "n_feature = train_data.shape[1]\n",
    "\n",
    "Y = np.zeros((n_train, n_class))\n",
    "for i in range(n_class):\n",
    "    Y[:, i] = (train_label == i).astype(int).ravel()\n",
    "\n",
    "# Logistic Regression with Gradient Descent\n",
    "W = np.zeros((n_feature + 1, n_class))\n",
    "initialWeights = np.zeros((n_feature + 1, 1))\n",
    "opts = {'maxiter': 100}\n",
    "for i in range(n_class):\n",
    "    labeli = Y[:, i].reshape(n_train, 1)\n",
    "    args = (train_data, labeli)\n",
    "    nn_params = minimize(blrObjFunction, initialWeights, jac=True, args=args, method='CG', options=opts)\n",
    "    W[:, i] = nn_params.x.reshape((n_feature + 1,))\n",
    "\n",
    "# Find the accuracy on Training Dataset\n",
    "predicted_label = blrPredict(W, train_data)\n",
    "print('\\nTraining set Accuracy:' + str(100 * np.mean((predicted_label == train_label).astype(float))) + '%')\n",
    "\n",
    "# Find the accuracy on Validation Dataset\n",
    "predicted_label = blrPredict(W, validation_data)\n",
    "print('Validation set Accuracy:' + str(100 * np.mean((predicted_label == validation_label).astype(float))) + '%')\n",
    "\n",
    "# Find the accuracy on Testing Dataset\n",
    "predicted_label = blrPredict(W, test_data)\n",
    "print('Testing set Accuracy:' + str(100 * np.mean((predicted_label == test_label).astype(float))) + '%')\n",
    "\n",
    "\"\"\"\n",
    "Script for Support Vector Machine\n",
    "\"\"\"\n",
    "\n",
    "print('\\n\\n--------------SVM-------------------\\n\\n')\n",
    "##################\n",
    "# YOUR CODE HERE #\n",
    "##################\n",
    "\n",
    "train_data_height = train_data.shape[0]\n",
    "train_data_width = train_data.shape[1]\n",
    "train_label_width = train_label.shape[1]\n",
    "random_indices = np.random.choice(range(train_data_height), 10000, replace=False)\n",
    "train_data_svm = np.empty((10000, train_data_width))\n",
    "train_label_svm = np.empty((10000, train_label_width), dtype=np.uintc)\n",
    "for index, randi in enumerate(random_indices):\n",
    "    train_data_svm[index] = train_data[randi]\n",
    "    train_label_svm[index] = train_label[randi]\n",
    "train_label_svm = train_label_svm.flatten()\n",
    "\n",
    "print(\"--For Linear--\")\n",
    "clf = svm.SVC(kernel=\"linear\")\n",
    "clf.fit(train_data_svm, train_label_svm)\n",
    "print(f\"SVM Training done\")\n",
    "predicted_label = clf.predict(train_data_svm)\n",
    "print(f\"\\nTrain set Accuracy: {100* np.mean((predicted_label == train_label_svm).astype(float))}%\")\n",
    "predicted_label = np.matrix(clf.predict(validation_data)).T\n",
    "print(f\"Validation set Accuracy: {100* np.mean((predicted_label == validation_label).astype(float))}%\")\n",
    "predicted_label = np.matrix(clf.predict(test_data)).T\n",
    "print(f\"Test set Accuracy: {100* np.mean((predicted_label == test_label).astype(float))}%\")\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"--For RBF with gamma 1--\")\n",
    "clf = svm.SVC(kernel=\"rbf\", gamma = 1.0)\n",
    "clf.fit(train_data_svm, train_label_svm)\n",
    "print(f\"SVM Training done\")\n",
    "predicted_label = clf.predict(train_data_svm)\n",
    "print(f\"\\nTrain set Accuracy: {100* np.mean((predicted_label == train_label_svm).astype(float))}%\")\n",
    "predicted_label = np.matrix(clf.predict(validation_data)).T\n",
    "print(f\"Validation set Accuracy: {100* np.mean((predicted_label == validation_label).astype(float))}%\")\n",
    "predicted_label = np.matrix(clf.predict(test_data)).T\n",
    "print(f\"Test set Accuracy: {100* np.mean((predicted_label == test_label).astype(float))}%\")\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"--For RBF--\")\n",
    "clf = svm.SVC()\n",
    "clf.fit(train_data_svm, train_label_svm)\n",
    "print(f\"SVM Training done\")\n",
    "predicted_label = clf.predict(train_data_svm)\n",
    "print(f\"\\nTrain set Accuracy: {100* np.mean((predicted_label == train_label_svm).astype(float))}%\")\n",
    "predicted_label = np.matrix(clf.predict(validation_data)).T\n",
    "print(f\"Validation set Accuracy: {100* np.mean((predicted_label == validation_label).astype(float))}%\")\n",
    "predicted_label = np.matrix(clf.predict(test_data)).T\n",
    "print(f\"Test set Accuracy: {100* np.mean((predicted_label == test_label).astype(float))}%\")\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"--For RBF with varying C--\")\n",
    "C = [1, 10, 20, 30, 40, 50, 60 ,70, 80, 90, 100]\n",
    "training_acc = []\n",
    "validation_acc = []\n",
    "test_acc = []\n",
    "\n",
    "for i in C:\n",
    "    print(f\"C={i}\")\n",
    "    clf = svm.SVC(kernel=\"rbf\", C=i)\n",
    "    clf.fit(train_data_svm, train_label_svm)\n",
    "    print(f\"SVM Training done\")\n",
    "    predicted_label = clf.predict(train_data_svm)\n",
    "    print(f\"\\nTrain set Accuracy: {100* np.mean((predicted_label == train_label_svm).astype(float))}%\")\n",
    "    training_acc.append(100* np.mean((predicted_label == train_label_svm).astype(float)))\n",
    "    predicted_label = np.matrix(clf.predict(validation_data)).T\n",
    "    print(f\"Validation set Accuracy: {100* np.mean((predicted_label == validation_label).astype(float))}%\")\n",
    "    validation_acc.append(100* np.mean((predicted_label == validation_label).astype(float)))\n",
    "    predicted_label = np.matrix(clf.predict(test_data)).T\n",
    "    print(f\"Test set Accuracy: {100* np.mean((predicted_label == test_label).astype(float))}%\")\n",
    "    test_acc.append(100* np.mean((predicted_label == test_label).astype(float)))\n",
    "    print(\"\\n\")\n",
    "\n",
    "\"\"\"\n",
    "Script for Extra Credit Part\n",
    "\"\"\"\n",
    "# FOR EXTRA CREDIT ONLY\n",
    "W_b = np.zeros((n_feature + 1, n_class))\n",
    "initialWeights_b = np.zeros((n_feature + 1, n_class))\n",
    "opts_b = {'maxiter': 100}\n",
    "\n",
    "args_b = (train_data, Y)\n",
    "nn_params = minimize(mlrObjFunction, initialWeights_b, jac=True, args=args_b, method='CG', options=opts_b)\n",
    "W_b = nn_params.x.reshape((n_feature + 1, n_class))\n",
    "\n",
    "# Find the accuracy on Training Dataset\n",
    "predicted_label_b = mlrPredict(W_b, train_data)\n",
    "print('\\nTraining set Accuracy:' + str(100 * np.mean((predicted_label_b == train_label).astype(float))) + '%')\n",
    "\n",
    "# Find the accuracy on Validation Dataset\n",
    "predicted_label_b = mlrPredict(W_b, validation_data)\n",
    "print('Validation set Accuracy:' + str(100 * np.mean((predicted_label_b == validation_label).astype(float))) + '%')\n",
    "\n",
    "# Find the accuracy on Testing Dataset\n",
    "predicted_label_b = mlrPredict(W_b, test_data)\n",
    "print('Testing set Accuracy:' + str(100 * np.mean((predicted_label_b == test_label).astype(float))) + '%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643421d5-ccf4-424d-ab56-2c3884e298f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
